{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.037729,
     "end_time": "2020-09-17T09:11:54.771010",
     "exception": false,
     "start_time": "2020-09-17T09:11:54.733281",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# **Analysing COVID19 Tweets!**\n",
    "# Hello Data Scientist, Welcome to my notebook!\n",
    "### <div align=\"left\">  Today we shall perform EDA, Clustering using KMeans and Sentiment analysis on the Tweets (related to #Covid19) to answer the following questions:\n",
    "Let us begin : \n",
    "</div>\n",
    "\n",
    "\n",
    "1. Which countries tweeted the most? (based on Generalised Content)\n",
    "> 1.1 which countries tweeted the most based on #Covid19 or tweets that are related to Corona/Covid19\n",
    "2. Was the tweet based on Corona if so how frequent was the tweet based on \n",
    "> 2.1 User Accounts / User Names\n",
    ">> 2.2 Country / Location \n",
    ">>> 2.3 Source / Device they tweeted from\n",
    "3. How much did the Verified profile people tweeted in the year 2019 and 2020 based out of the country?\n",
    "4. Monthly tweet trend of 2019 vs 2020\n",
    "5. Who are the top 40 people/accounts that has most followers and their tweet count?\n",
    "6. What are the most common words in the tweets?\n",
    "7. Clustering the most commonly used tweet words into 5 groups\n",
    "> 7.1 Group 1\n",
    ">> 7.2 Group 2\n",
    ">>> 7.3 Group 3\n",
    ">>>> 7.4 Group 4\n",
    ">>>>> 7.5 Group 5\n",
    "8. What are the positive and negative sentiment words?\n",
    "> Word-cloud of Positive and Negative twitter bird\n",
    "9. What are the top 10 countries that tweeted with positive sentiments?\n",
    "10. How is the data (countries, people, tweet content, month) distributed with sentiments?\n",
    "> 10.1 In the Year 2019\n",
    ">> 10.2 In the Year 2020\n",
    "\n",
    "<div align=\"center\"> \n",
    "    If you loved my notebook, please upvote to encourage me! \n",
    "</div> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.036426,
     "end_time": "2020-09-17T09:11:54.844069",
     "exception": false,
     "start_time": "2020-09-17T09:11:54.807643",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "###### Pre-requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-09-17T09:11:54.924429Z",
     "iopub.status.busy": "2020-09-17T09:11:54.923616Z",
     "iopub.status.idle": "2020-09-17T09:11:54.927141Z",
     "shell.execute_reply": "2020-09-17T09:11:54.927593Z"
    },
    "papermill": {
     "duration": 0.04695,
     "end_time": "2020-09-17T09:11:54.927727",
     "exception": false,
     "start_time": "2020-09-17T09:11:54.880777",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nnumpy library\\npandas library\\nmatplot library\\nplotly library\\nwordcloud library\\nnltk library\\nPIL library\\nseaborn library\\nre library\\nscikit learn library\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "numpy library\n",
    "pandas library\n",
    "matplot library\n",
    "plotly library\n",
    "wordcloud library\n",
    "nltk library\n",
    "PIL library\n",
    "seaborn library\n",
    "re library\n",
    "scikit learn library\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.036975,
     "end_time": "2020-09-17T09:11:55.002114",
     "exception": false,
     "start_time": "2020-09-17T09:11:54.965139",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Import Statements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-09-17T09:11:55.083346Z",
     "iopub.status.busy": "2020-09-17T09:11:55.082758Z",
     "iopub.status.idle": "2020-09-17T09:12:00.038630Z",
     "shell.execute_reply": "2020-09-17T09:12:00.039363Z"
    },
    "papermill": {
     "duration": 5.000652,
     "end_time": "2020-09-17T09:12:00.039558",
     "exception": false,
     "start_time": "2020-09-17T09:11:55.038906",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\riaz\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\n#from sklearn.model_selection import train_test_split\\nfrom sklearn import preprocessing\\nfrom sklearn import model_selection\\nfrom sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\\nfrom collections import Counter\\n\\nimport tensorflow as tf\\nfrom tensorflow.keras.models import Sequential\\nfrom tensorflow.keras.layers import Dense,Activation\\nfrom tensorflow.keras.callbacks import EarlyStopping\\nfrom tensorflow.keras.preprocessing.text import Tokenizer\\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import plotly.figure_factory as ff\n",
    "from plotly.colors import n_colors\n",
    "from plotly.subplots import make_subplots\n",
    "from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\n",
    "\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "#stop=set(stopwords.words('english'))\n",
    "\n",
    "from PIL import Image\n",
    "import seaborn as sns\n",
    "import re\n",
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "import chardet\n",
    "\n",
    "# tried building a nlp model for sentiments, but it was a time killer as i had prior exposure only on ANN and CNN \n",
    "#So made use of prebuild nlp model that predicts sentiments\n",
    "\n",
    "'''\n",
    "#from sklearn.model_selection import train_test_split\n",
    "from sklearn import preprocessing\n",
    "from sklearn import model_selection\n",
    "from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\n",
    "from collections import Counter\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense,Activation\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.059541,
     "end_time": "2020-09-17T09:12:00.162113",
     "exception": false,
     "start_time": "2020-09-17T09:12:00.102572",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Filepath of the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-09-17T09:12:00.294448Z",
     "iopub.status.busy": "2020-09-17T09:12:00.293669Z",
     "iopub.status.idle": "2020-09-17T09:12:01.601221Z",
     "shell.execute_reply": "2020-09-17T09:12:01.600746Z"
    },
    "papermill": {
     "duration": 1.378272,
     "end_time": "2020-09-17T09:12:01.601317",
     "exception": false,
     "start_time": "2020-09-17T09:12:00.223045",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../input/covid19-tweets/covid19_tweets.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-1bb38460b6b2>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0minfo_path\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34mr'../input/covid-19-tweet-supporting-files/columns.txt'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;31m#sentiment_filepath = r'C:/Users/riaz/learning/user_sentiment.csv'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[1;31m#sentiment = pd.read_csv(sentiment_filepath,encoding = 'latin',header=None,names=['target','id','time','query','usr','text'])\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0minfo\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minfo_path\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdelimiter\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'->'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mnames\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'title'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'description'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\python-cvcourse\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[0;32m    684\u001b[0m     )\n\u001b[0;32m    685\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 686\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    687\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    688\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\python-cvcourse\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    450\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    451\u001b[0m     \u001b[1;31m# Create the parser.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 452\u001b[1;33m     \u001b[0mparser\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfp_or_buf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    453\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    454\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\python-cvcourse\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m    934\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"has_index_names\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"has_index_names\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    935\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 936\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    937\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    938\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\python-cvcourse\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[1;34m(self, engine)\u001b[0m\n\u001b[0;32m   1166\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"c\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1167\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"c\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1168\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1169\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1170\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"python\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\python-cvcourse\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, src, **kwds)\u001b[0m\n\u001b[0;32m   1996\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"usecols\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1997\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1998\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1999\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munnamed_cols\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munnamed_cols\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2000\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../input/covid19-tweets/covid19_tweets.csv'"
     ]
    }
   ],
   "source": [
    "filepath = r'C:/Users/riaz/Documents/GitHub/myPrograms/RealTime_problems/Covid19_tweets/covid19_tweets.csv'\n",
    "info_path = r'C:/Users/riaz/Documents/GitHub/myPrograms/RealTime_problems/Covid19_tweets/columns.txt'\n",
    "#sentiment_filepath = r'C:/Users/riaz/learning/user_sentiment.csv'\n",
    "df = pd.read_csv(filepath)\n",
    "#sentiment = pd.read_csv(sentiment_filepath,encoding = 'latin',header=None,names=['target','id','time','query','usr','text'])\n",
    "info = pd.read_csv(info_path,delimiter='->',names=['title','description'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.072234,
     "end_time": "2020-09-17T09:12:01.743205",
     "exception": false,
     "start_time": "2020-09-17T09:12:01.670971",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Correlation of given datas with heatmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-09-17T09:12:01.883745Z",
     "iopub.status.busy": "2020-09-17T09:12:01.882787Z",
     "iopub.status.idle": "2020-09-17T09:12:02.325514Z",
     "shell.execute_reply": "2020-09-17T09:12:02.322861Z"
    },
    "papermill": {
     "duration": 0.515658,
     "end_time": "2020-09-17T09:12:02.325678",
     "exception": false,
     "start_time": "2020-09-17T09:12:01.810020",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(5,3))\n",
    "sns.heatmap(df.corr(),annot=True,linecolor='white',linewidths=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.068026,
     "end_time": "2020-09-17T09:12:02.460759",
     "exception": false,
     "start_time": "2020-09-17T09:12:02.392733",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "#### Plotting unique features in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-09-17T09:12:02.566338Z",
     "iopub.status.busy": "2020-09-17T09:12:02.565564Z",
     "iopub.status.idle": "2020-09-17T09:12:02.568935Z",
     "shell.execute_reply": "2020-09-17T09:12:02.569468Z"
    },
    "papermill": {
     "duration": 0.068075,
     "end_time": "2020-09-17T09:12:02.569624",
     "exception": false,
     "start_time": "2020-09-17T09:12:02.501549",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def unique_features(data_frame):\n",
    "    unique_dataframe = pd.DataFrame()\n",
    "    unique_dataframe['features'] = data_frame.columns\n",
    "    uniques = []\n",
    "    for col in data_frame.columns:\n",
    "        u = data_frame[col].nunique()\n",
    "        uniques.append(u)\n",
    "    unique_dataframe['uniques'] = uniques\n",
    "    return unique_dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-09-17T09:12:02.703297Z",
     "iopub.status.busy": "2020-09-17T09:12:02.702548Z",
     "iopub.status.idle": "2020-09-17T09:12:03.365578Z",
     "shell.execute_reply": "2020-09-17T09:12:03.365037Z"
    },
    "papermill": {
     "duration": 0.736237,
     "end_time": "2020-09-17T09:12:03.365710",
     "exception": false,
     "start_time": "2020-09-17T09:12:02.629473",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "features = unique_features(df)\n",
    "#print(features)\n",
    "\n",
    "plt.figure(figsize=(10,7))\n",
    "features = features.sort_values(by='uniques',ascending=False)\n",
    "sns.barplot(x='uniques',y='features',data=features,palette='Dark2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.046325,
     "end_time": "2020-09-17T09:12:03.460703",
     "exception": false,
     "start_time": "2020-09-17T09:12:03.414378",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### CONVERTING TIME STAMPS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-09-17T09:12:03.612296Z",
     "iopub.status.busy": "2020-09-17T09:12:03.611402Z",
     "iopub.status.idle": "2020-09-17T09:12:06.986223Z",
     "shell.execute_reply": "2020-09-17T09:12:06.986715Z"
    },
    "papermill": {
     "duration": 3.483247,
     "end_time": "2020-09-17T09:12:06.986863",
     "exception": false,
     "start_time": "2020-09-17T09:12:03.503616",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df['user_created'] = pd.to_datetime(df['user_created'])\n",
    "df['only_date'] = pd.to_datetime(df['user_created']).dt.date\n",
    "df['created_year'] = df['user_created'].apply(lambda date : date.year)\n",
    "df['created_month'] = df['user_created'].apply(lambda date : date.month)\n",
    "df['created_day'] = df['user_created'].apply(lambda date : date.day)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.040274,
     "end_time": "2020-09-17T09:12:07.068123",
     "exception": false,
     "start_time": "2020-09-17T09:12:07.027849",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Data cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.04048,
     "end_time": "2020-09-17T09:12:07.149145",
     "exception": false,
     "start_time": "2020-09-17T09:12:07.108665",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "###### Checking for empty objects / nan "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-09-17T09:12:07.351454Z",
     "iopub.status.busy": "2020-09-17T09:12:07.350663Z",
     "iopub.status.idle": "2020-09-17T09:12:07.481565Z",
     "shell.execute_reply": "2020-09-17T09:12:07.481133Z"
    },
    "papermill": {
     "duration": 0.292384,
     "end_time": "2020-09-17T09:12:07.481680",
     "exception": false,
     "start_time": "2020-09-17T09:12:07.189296",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "data = df.isnull().sum().sort_values(ascending=False)[:4]\n",
    "plt.figure(figsize=(3,3))\n",
    "data.plot(kind='barh',grid=False,sort_columns=True,title='total_missing_values')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.041983,
     "end_time": "2020-09-17T09:12:07.565325",
     "exception": false,
     "start_time": "2020-09-17T09:12:07.523342",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "###### Grouping / cleaning unknown dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-09-17T09:12:07.690525Z",
     "iopub.status.busy": "2020-09-17T09:12:07.689520Z",
     "iopub.status.idle": "2020-09-17T09:12:07.729052Z",
     "shell.execute_reply": "2020-09-17T09:12:07.728599Z"
    },
    "papermill": {
     "duration": 0.122155,
     "end_time": "2020-09-17T09:12:07.729148",
     "exception": false,
     "start_time": "2020-09-17T09:12:07.606993",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df['user_location'].fillna('Unknown', inplace=True)\n",
    "df['user_description'].fillna('Unknown', inplace=True)\n",
    "df['source'].fillna('Unknown', inplace=True)\n",
    "df['hashtags'].fillna('None', inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.041451,
     "end_time": "2020-09-17T09:12:07.812202",
     "exception": false,
     "start_time": "2020-09-17T09:12:07.770751",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "###### Cleaning and Extracting  country name from \"user_ locations\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-09-17T09:12:07.914514Z",
     "iopub.status.busy": "2020-09-17T09:12:07.913964Z",
     "iopub.status.idle": "2020-09-17T09:12:11.682898Z",
     "shell.execute_reply": "2020-09-17T09:12:11.682340Z"
    },
    "papermill": {
     "duration": 3.828309,
     "end_time": "2020-09-17T09:12:11.683008",
     "exception": false,
     "start_time": "2020-09-17T09:12:07.854699",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#splitting a string into 2 words, where the first word corresponds to city name and second word to the country name\n",
    "#this acts only if the given str has a ',' in it else it would return the same\n",
    "df['country'] = df['user_location'].apply(lambda x: x.split(\",\")[-1].strip() if (\",\" in x) else x)\n",
    "df['city'] = df['user_location'].apply(lambda x: x.split(\",\")[0].strip() if (\",\" in x) else x)\n",
    "\n",
    "#replacing the two digit US city names with USA except UK and EU\n",
    "df['country'] = df['country'].apply(lambda x: 'USA' if (len(x.lower().strip())<3) and ((x!='uk')|(x!='eu')) else x)\n",
    "#replacing lower case country names with standard ones\n",
    "df['country'] = df['country'].apply(lambda x: 'USA' if x.lower().strip() in (\"united states,Alabama, Alaska, American Samoa, Arizona, Arkansas, California, Colorado, Connecticut, Delaware, District of Columbia, Florida, Georgia, Guam, Hawaii, Idaho, Illinois, Indiana, Iowa, Kansas, Kentucky, Louisiana, Maine, Maryland, Massachusetts, Michigan, Minnesota, Mississippi, Missouri, Montana, Nebraska, Nevada, New Hampshire, New Jersey, New Mexico, New York, North Carolina, North Dakota, Northern Mariana Islands, Ohio, Oklahoma, Oregon, Pennsylvania, Puerto Rico, Rhode Island, South Carolina, South Dakota, Tennessee, Texas, U.S. Virgin Islands, Utah, Vermont, Virginia, Washington, West Virginia, Wisconsin, Wyoming california, texas, usa,new york, us\") else x)\n",
    "df['country'] = df['country'].apply(lambda x: 'Canada' if x.lower().strip() in (\"ontario,toronto,quebec,montreal,quebec city,vancouver\") else x)\n",
    "df['country'] = df['country'].apply(lambda x: 'UK' if x.lower().strip() in (\"united kingdom,london, england,uk, britain,great britain\") else x)\n",
    "df['country'] = df['country'].apply(lambda x: 'India' if x.lower().strip() in (\"india,mumbai,tamil nadu,chennai,karnataka,bengaluru,kerala,thiruvanandhipuram,kochi,patna,delhi,new delhi,uttar pradesh,andhra pradesh,telengana,vishakapatinam,hyderabad,himachal pradesh,goa,jammu,jammu and kashmir,ladhak\") else x)\n",
    "df['country'] = df['country'].apply(lambda x: 'N/A' if x.lower().strip() in (\"worldwide, earth, global\") else x)\n",
    "\n",
    "\n",
    "#if country name is found in city name,\n",
    "country_list = ['finland','netherlands','ireland','sweden','germany','denmark','switzerland','norway','france','spain','canada','bulgaria','belgium','estonia','uk','luxembourg','newzealand','austria','italy','australia','latvia','cyprus','singapore','japan','northmacedonia','southkorea','moldova','slovakia','romania','portugal','poland','czechrepublic','slovenia','costarica','chile','iceland','lithuania','georgia','hungary','usa','russia','greece','india','malaysia','armenia','southafrica']\n",
    "\n",
    "def checker(x):\n",
    "    #checking for city names in country list\n",
    "    if x['city'].lower().strip() in country_list:\n",
    "        #if city name is not same as country name but is in country list\n",
    "        if x['city'].lower().strip() != x['country'].lower().strip():\n",
    "            return x['city']\n",
    "        else:\n",
    "            return x['country']\n",
    "    else:\n",
    "        return x['country']\n",
    "\n",
    "df['country'] = df[['city','country']].apply(checker,axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.041383,
     "end_time": "2020-09-17T09:12:11.768995",
     "exception": false,
     "start_time": "2020-09-17T09:12:11.727612",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "###### Hashtage cleaner fuction, which groups tweets that are specific to Covid 19 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-09-17T09:12:11.862892Z",
     "iopub.status.busy": "2020-09-17T09:12:11.862058Z",
     "iopub.status.idle": "2020-09-17T09:12:12.271881Z",
     "shell.execute_reply": "2020-09-17T09:12:12.271370Z"
    },
    "papermill": {
     "duration": 0.461031,
     "end_time": "2020-09-17T09:12:12.271988",
     "exception": false,
     "start_time": "2020-09-17T09:12:11.810957",
     "status": "completed"
    },
    "run_control": {
     "marked": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "corona_keys = ['covid19','covid_19','covid','pandemic','covid-19','corona','virus','coronavirus','coronavirusupdates']\n",
    "\n",
    "def hashtag_grouper(a):\n",
    "    b = a.replace(\"\\'\",\"\").strip().strip(\"[\").strip(\"]\").lower()\n",
    "    c = b.split(',')\n",
    "    for item in c:\n",
    "        if item in corona_keys:\n",
    "            return 'Covid 19'\n",
    "        else:\n",
    "            return item\n",
    "\n",
    "df['category'] = df['hashtags'].apply(hashtag_grouper)\n",
    "df['covid'] = df['category'].apply(lambda x: True if x == 'Covid 19' else False)\n",
    "coronadf = df[df['covid']>0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.047309,
     "end_time": "2020-09-17T09:12:12.366102",
     "exception": false,
     "start_time": "2020-09-17T09:12:12.318793",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "##### creating a dataFrame that has count of  total number of tweets per country"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-09-17T09:12:12.515487Z",
     "iopub.status.busy": "2020-09-17T09:12:12.514554Z",
     "iopub.status.idle": "2020-09-17T09:12:12.542712Z",
     "shell.execute_reply": "2020-09-17T09:12:12.542245Z"
    },
    "papermill": {
     "duration": 0.128825,
     "end_time": "2020-09-17T09:12:12.542804",
     "exception": false,
     "start_time": "2020-09-17T09:12:12.413979",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#dataFrame of countries that tweeted general content\n",
    "location_count = pd.DataFrame(df['country'].value_counts())\n",
    "location_count.reset_index(inplace=True)\n",
    "location_count = location_count.set_axis(['country', 'count'], axis=1)\n",
    "location_count = location_count.sort_values(by='count',ascending=False)\n",
    "\n",
    "\n",
    "#dataFrame of countries that tweeted about covid 19\n",
    "location_count2 = pd.DataFrame(coronadf['country'].value_counts())\n",
    "location_count2.reset_index(inplace=True)\n",
    "location_count2 = location_count2.set_axis(['country', 'cv_count'], axis=1)\n",
    "location_count2 = location_count2.sort_values(by='cv_count',ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.042579,
     "end_time": "2020-09-17T09:12:12.628036",
     "exception": false,
     "start_time": "2020-09-17T09:12:12.585457",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 1. Countries that tweeted the most on generalised content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-09-17T09:12:12.728004Z",
     "iopub.status.busy": "2020-09-17T09:12:12.727074Z",
     "iopub.status.idle": "2020-09-17T09:12:13.741742Z",
     "shell.execute_reply": "2020-09-17T09:12:13.742756Z"
    },
    "papermill": {
     "duration": 1.07169,
     "end_time": "2020-09-17T09:12:13.742922",
     "exception": false,
     "start_time": "2020-09-17T09:12:12.671232",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "count_graph = px.bar(x='country',y='count',data_frame=location_count[1:16],color='country',\n",
    "                    labels={'x':'Countries','y':'Counts'},title='Generalised Tweets')\n",
    "count_graph.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.044099,
     "end_time": "2020-09-17T09:12:13.842069",
     "exception": false,
     "start_time": "2020-09-17T09:12:13.797970",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 1.1 Now, we shall see countries that specifically tweeted the most about Covid 19 and their tweet counts respectively"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-09-17T09:12:14.017614Z",
     "iopub.status.busy": "2020-09-17T09:12:14.012361Z",
     "iopub.status.idle": "2020-09-17T09:12:14.026395Z",
     "shell.execute_reply": "2020-09-17T09:12:14.026826Z"
    },
    "papermill": {
     "duration": 0.139001,
     "end_time": "2020-09-17T09:12:14.026943",
     "exception": false,
     "start_time": "2020-09-17T09:12:13.887942",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "count_graph2 = px.bar(x=location_count2['country'][1:16],y=location_count2['cv_count'][1:16],\n",
    "                      color=location_count2['country'][1:16],labels={'x':'Countries','y':'Counts'},\n",
    "                      title='Tweets based on corona/Covid 19')\n",
    "count_graph2.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.044231,
     "end_time": "2020-09-17T09:12:14.116261",
     "exception": false,
     "start_time": "2020-09-17T09:12:14.072030",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### 2. EDA on tweets from the year 2019 (i.e. 2019 + 2020) based on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-09-17T09:12:14.217766Z",
     "iopub.status.busy": "2020-09-17T09:12:14.217169Z",
     "iopub.status.idle": "2020-09-17T09:12:14.221363Z",
     "shell.execute_reply": "2020-09-17T09:12:14.220909Z"
    },
    "papermill": {
     "duration": 0.060579,
     "end_time": "2020-09-17T09:12:14.221448",
     "exception": false,
     "start_time": "2020-09-17T09:12:14.160869",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def graph(data, feature, title, pallete):\n",
    "    f, ax = plt.subplots(1,1, figsize=(18,6))\n",
    "    total = float(len(df))\n",
    "    if feature == 'user_location' or 'country' or 'city' or 'user_names':\n",
    "        g = sns.countplot(data[feature],hue= data['covid'],order = data[feature].value_counts().index[1:11], palette=pallete)  \n",
    "    elif feature == 'source':\n",
    "        g = sns.countplot(data[feature],hue= data['covid'],order = data[feature].value_counts().index[:16], palette=pallete)\n",
    "    else:\n",
    "        g = sns.countplot(data[feature],hue= data['covid'], order = data[feature].value_counts().index[0:11], palette=pallete)\n",
    "    g.set_title(\"Number and percentage of {}\".format(title))\n",
    "    for p in ax.patches:\n",
    "        height = p.get_height()\n",
    "        ax.text(p.get_x()+p.get_width()/2.,\n",
    "                height + 3,\n",
    "                '{:1.2f}%'.format(100*height/total),\n",
    "                ha=\"center\") \n",
    "\n",
    "    plt.title('Counts & Percentage representation of {} that were actually tweeting about Corona'.format(feature))\n",
    "    plt.ylabel('Frequency', fontsize=12)\n",
    "    plt.xlabel(title, fontsize=12)\n",
    "    plt.xticks(rotation=90)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.04486,
     "end_time": "2020-09-17T09:12:14.310821",
     "exception": false,
     "start_time": "2020-09-17T09:12:14.265961",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### 2.1 This plot symbolizes how frequent did the top user tweeted and whether the content was related to Covid19"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-09-17T09:12:14.407934Z",
     "iopub.status.busy": "2020-09-17T09:12:14.407100Z",
     "iopub.status.idle": "2020-09-17T09:12:14.782486Z",
     "shell.execute_reply": "2020-09-17T09:12:14.782941Z"
    },
    "papermill": {
     "duration": 0.426487,
     "end_time": "2020-09-17T09:12:14.783061",
     "exception": false,
     "start_time": "2020-09-17T09:12:14.356574",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "graph(df[df.created_year > 2019 ], 'user_name', 'User Names','CMRmap')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.047079,
     "end_time": "2020-09-17T09:12:14.876674",
     "exception": false,
     "start_time": "2020-09-17T09:12:14.829595",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### 2.2 This plot symbolizes Where did the most tweets come from and whether the content was related to Covid19"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-09-17T09:12:14.974864Z",
     "iopub.status.busy": "2020-09-17T09:12:14.974014Z",
     "iopub.status.idle": "2020-09-17T09:12:15.244846Z",
     "shell.execute_reply": "2020-09-17T09:12:15.245260Z"
    },
    "papermill": {
     "duration": 0.322327,
     "end_time": "2020-09-17T09:12:15.245399",
     "exception": false,
     "start_time": "2020-09-17T09:12:14.923072",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "graph(df[df.created_year > 2019 ], 'country', 'User Locations', 'gist_rainbow')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.047499,
     "end_time": "2020-09-17T09:12:15.341437",
     "exception": false,
     "start_time": "2020-09-17T09:12:15.293938",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### 2.3 This plot symbolizes from which devices did the tweets originated and whether the content was related to Covid19"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-09-17T09:12:15.481948Z",
     "iopub.status.busy": "2020-09-17T09:12:15.481280Z",
     "iopub.status.idle": "2020-09-17T09:12:15.766228Z",
     "shell.execute_reply": "2020-09-17T09:12:15.766669Z"
    },
    "papermill": {
     "duration": 0.37765,
     "end_time": "2020-09-17T09:12:15.766791",
     "exception": false,
     "start_time": "2020-09-17T09:12:15.389141",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "graph(df[df.created_year > 2019 ], 'source','Source', 'plasma_r')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.051856,
     "end_time": "2020-09-17T09:12:15.867798",
     "exception": false,
     "start_time": "2020-09-17T09:12:15.815942",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### 3. This EDA show us how did the \"verified_Profile\" users reacted from the top 5 countries in the year 2019 and 2020"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-09-17T09:12:15.972484Z",
     "iopub.status.busy": "2020-09-17T09:12:15.971708Z",
     "iopub.status.idle": "2020-09-17T09:12:16.107184Z",
     "shell.execute_reply": "2020-09-17T09:12:16.106734Z"
    },
    "papermill": {
     "duration": 0.190657,
     "end_time": "2020-09-17T09:12:16.107285",
     "exception": false,
     "start_time": "2020-09-17T09:12:15.916628",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "verified_corona = df[ (df.user_verified == True) & (df.covid == True) & (df.country == ('USA')) | \n",
    "             (df.country == 'India') | (df.country == 'Australia') | (df.country =='UK') | (df.country =='Canada') ]  \n",
    "\n",
    "verified_corona_20 = verified_corona[verified_corona.created_year == 2020]\n",
    "verified_corona = verified_corona[verified_corona.created_year == 2019]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-09-17T09:12:16.224974Z",
     "iopub.status.busy": "2020-09-17T09:12:16.224068Z",
     "iopub.status.idle": "2020-09-17T09:12:16.814247Z",
     "shell.execute_reply": "2020-09-17T09:12:16.814729Z"
    },
    "papermill": {
     "duration": 0.658359,
     "end_time": "2020-09-17T09:12:16.814859",
     "exception": false,
     "start_time": "2020-09-17T09:12:16.156500",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "print('Number of tweets from top 5 countries')\n",
    "f, axes = plt.subplots(1, 2,figsize=(16,6))\n",
    "axes[0].set_title(\"Year 2019\")\n",
    "axes[1].set_title(\"Year 2020\")\n",
    "order_type = sorted(verified_corona['created_month'].unique())\n",
    "sns.countplot(x=verified_corona.created_month, hue=verified_corona.country,palette=\"Dark2\",order=order_type,ax=axes[0])\n",
    "order_type = sorted(verified_corona_20['created_month'].unique())\n",
    "sns.countplot(x=verified_corona_20.created_month, hue=verified_corona_20.country,palette=\"Dark2\",order=order_type,ax=axes[1])\n",
    "\n",
    "f.tight_layout()\n",
    "plt.show()\n",
    "#'Number of verified profile tweets in the year 2019 vs 2020'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.051532,
     "end_time": "2020-09-17T09:12:16.917566",
     "exception": false,
     "start_time": "2020-09-17T09:12:16.866034",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### 4. Finding a pattern/Spike in tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.051178,
     "end_time": "2020-09-17T09:12:17.020159",
     "exception": false,
     "start_time": "2020-09-17T09:12:16.968981",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "##### From this **Line Chart**  we could see quite a spike in number of tweets(February,2020 to March,2020), that is when the WHO declared Global Pandemic and many nations started security measurments such as Lockdowns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-09-17T09:12:17.133752Z",
     "iopub.status.busy": "2020-09-17T09:12:17.132875Z",
     "iopub.status.idle": "2020-09-17T09:12:17.181469Z",
     "shell.execute_reply": "2020-09-17T09:12:17.181040Z"
    },
    "papermill": {
     "duration": 0.110577,
     "end_time": "2020-09-17T09:12:17.181572",
     "exception": false,
     "start_time": "2020-09-17T09:12:17.070995",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "tweet_from_2019 = df[df['created_year'] == 2019]\n",
    "tweet_from_2020 = df[df['created_year'] == 2020]\n",
    "tweets_month_19 =  tweet_from_2019['created_month'].value_counts().to_frame().reset_index().rename(columns={'index':'month','created_month':'count'})   \n",
    "tweets_month_20 =  tweet_from_2020['created_month'].value_counts().to_frame().reset_index().rename(columns={'index':'month','created_month':'count'})   \n",
    "tweets_month_19 = tweets_month_19.sort_values('month',ascending=True)\n",
    "tweets_month_20 = tweets_month_20.sort_values('month',ascending=True)\n",
    "\n",
    "fig=go.Figure()\n",
    "fig.add_trace(go.Scatter(x=tweets_month_19['month'], y=tweets_month_19['count'],\n",
    "                         mode='markers+lines',marker_color='firebrick',\n",
    "                         name='Year 2019',line = dict(color='grey', width=4, dash='dot')))\n",
    "\n",
    "#fig.update_layout(title_text='Tweets per Month and Date in the year 2019 and 2020 ',template=\"plotly\", title_x=0.5)\n",
    "fig.add_trace(go.Scatter(x=tweets_month_20['month'], y=tweets_month_20['count'],\n",
    "                         mode='markers+lines',marker_color='darkred',name='Year 2020',\n",
    "                        line=dict(color='orange', width=4,dash='longdashdot')))\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.050876,
     "end_time": "2020-09-17T09:12:17.284366",
     "exception": false,
     "start_time": "2020-09-17T09:12:17.233490",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### 5. Most popular user with thier tweet count and followers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-09-17T09:12:17.457606Z",
     "iopub.status.busy": "2020-09-17T09:12:17.457039Z",
     "iopub.status.idle": "2020-09-17T09:12:18.283806Z",
     "shell.execute_reply": "2020-09-17T09:12:18.284556Z"
    },
    "papermill": {
     "duration": 0.946054,
     "end_time": "2020-09-17T09:12:18.284706",
     "exception": false,
     "start_time": "2020-09-17T09:12:17.338652",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "ds = df['user_name'].value_counts().reset_index()\n",
    "ds.columns = ['user_name', 'tweets_count']\n",
    "ds = ds.sort_values(['tweets_count'])\n",
    "df = pd.merge(df, ds, on='user_name')\n",
    "\n",
    "data = df.sort_values('user_followers', ascending=False)\n",
    "data = data.drop_duplicates(subset='user_name', keep=\"first\")\n",
    "data = data[['user_name', 'user_followers', 'tweets_count']]\n",
    "data = data.sort_values('user_followers')\n",
    "fig = px.bar(data.tail(40), x=\"user_followers\", y=\"user_name\",color='tweets_count',orientation='h', \n",
    "             title='Top 40 users by number of followers', width=1000, height=1000,template=\"plotly_dark\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.051446,
     "end_time": "2020-09-17T09:12:18.388282",
     "exception": false,
     "start_time": "2020-09-17T09:12:18.336836",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "###### Preprocession text from tweeted content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-09-17T09:12:18.500890Z",
     "iopub.status.busy": "2020-09-17T09:12:18.500025Z",
     "iopub.status.idle": "2020-09-17T09:12:18.504939Z",
     "shell.execute_reply": "2020-09-17T09:12:18.505317Z"
    },
    "papermill": {
     "duration": 0.06549,
     "end_time": "2020-09-17T09:12:18.505446",
     "exception": false,
     "start_time": "2020-09-17T09:12:18.439956",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def remove_tag(string):\n",
    "    text=re.sub('<.*?>','',string)\n",
    "    return text\n",
    "def remove_mention(text):\n",
    "    line=re.sub(r'@\\w+','',text)\n",
    "    return line\n",
    "def remove_hash(text):\n",
    "    line=re.sub(r'#\\w+','',text)\n",
    "    return line\n",
    "\n",
    "def remove_newline(string):\n",
    "    text=re.sub('\\n','',string)\n",
    "    return text\n",
    "def remove_url(string): \n",
    "    text = re.sub('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+','',string)\n",
    "    return text\n",
    "def remove_number(text):\n",
    "    line=re.sub(r'[0-9]+','',text)\n",
    "    return line\n",
    "def remove_punct(text):\n",
    "    line = re.sub(r'[!\"\\$%&\\'()*+,\\-.\\/:;=#@?\\[\\\\\\]^_`{|}~]*','',text)\n",
    "    #string=\"\".join(line)\n",
    "    return line\n",
    "def text_strip(string):\n",
    "    line=re.sub('\\s{2,}', ' ', string.strip())\n",
    "    return line\n",
    "def remove_thi_amp_ha_words(string):\n",
    "    line=re.sub(r'\\bamp\\b|\\bthi\\b|\\bha\\b',' ',string)\n",
    "    return line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-09-17T09:12:18.647892Z",
     "iopub.status.busy": "2020-09-17T09:12:18.647207Z",
     "iopub.status.idle": "2020-09-17T09:12:30.326796Z",
     "shell.execute_reply": "2020-09-17T09:12:30.326276Z"
    },
    "papermill": {
     "duration": 11.768468,
     "end_time": "2020-09-17T09:12:30.326914",
     "exception": false,
     "start_time": "2020-09-17T09:12:18.558446",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df['clean_tweet'] =  df['text'].str.lower()\n",
    "df['clean_tweet'] = df['clean_tweet'].apply(lambda x:remove_tag(str(x)))\n",
    "df['clean_tweet'] = df['clean_tweet'].apply(lambda x:remove_mention(str(x)))\n",
    "df['clean_tweet'] = df['clean_tweet'].apply(lambda x:remove_hash(str(x)))\n",
    "df['clean_tweet'] = df['clean_tweet'].apply(lambda x:remove_newline(x))\n",
    "df['clean_tweet'] = df['clean_tweet'].apply(lambda x:remove_url(x))\n",
    "df['clean_tweet'] = df['clean_tweet'].apply(lambda x:remove_number(x))\n",
    "df['clean_tweet'] = df['clean_tweet'].apply(lambda x:remove_punct(x))\n",
    "df['clean_tweet'] = df['clean_tweet'].apply(lambda x:remove_thi_amp_ha_words(x))\n",
    "df['clean_tweet'] = df['clean_tweet'].apply(lambda x:text_strip(x))\n",
    "\n",
    "df['text_length']=df['clean_tweet'].str.split().map(lambda x: len(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.051481,
     "end_time": "2020-09-17T09:12:30.431025",
     "exception": false,
     "start_time": "2020-09-17T09:12:30.379544",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Word clouds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-09-17T09:12:30.541980Z",
     "iopub.status.busy": "2020-09-17T09:12:30.541387Z",
     "iopub.status.idle": "2020-09-17T09:12:30.544910Z",
     "shell.execute_reply": "2020-09-17T09:12:30.544461Z"
    },
    "papermill": {
     "duration": 0.062078,
     "end_time": "2020-09-17T09:12:30.544994",
     "exception": false,
     "start_time": "2020-09-17T09:12:30.482916",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def build_wordcloud(var, title):\n",
    "    wordcloud = WordCloud(background_color='black',colormap=\"bwr\", \n",
    "                          stopwords=set(STOPWORDS),max_words=80, min_font_size= 8,\n",
    "                          max_font_size=40, random_state=666).generate(str(var))\n",
    "\n",
    "    fig = plt.figure(1, figsize=(15,15))\n",
    "    plt.axis('off')\n",
    "    fig.suptitle(title, fontsize=16)\n",
    "    fig.subplots_adjust(top=2.3)\n",
    "\n",
    "    plt.imshow(wordcloud,interpolation='bilinear')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.052845,
     "end_time": "2020-09-17T09:12:30.649844",
     "exception": false,
     "start_time": "2020-09-17T09:12:30.596999",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### 6. Creating a word cloud for the most popular words used"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.053222,
     "end_time": "2020-09-17T09:12:30.756027",
     "exception": false,
     "start_time": "2020-09-17T09:12:30.702805",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "###### As predicted a  huge  propotion of words are about Covid!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-09-17T09:12:30.877449Z",
     "iopub.status.busy": "2020-09-17T09:12:30.876476Z",
     "iopub.status.idle": "2020-09-17T09:12:31.126371Z",
     "shell.execute_reply": "2020-09-17T09:12:31.126805Z"
    },
    "papermill": {
     "duration": 0.318158,
     "end_time": "2020-09-17T09:12:31.126928",
     "exception": false,
     "start_time": "2020-09-17T09:12:30.808770",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "build_wordcloud(df['clean_tweet'], 'Popular keywords used in all tweets')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.058327,
     "end_time": "2020-09-17T09:12:31.244268",
     "exception": false,
     "start_time": "2020-09-17T09:12:31.185941",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### 7. Clustering the most commonly used word using KMeans\n",
    " Cheers! 🥂 learnt from ( https://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-09-17T09:12:31.388960Z",
     "iopub.status.busy": "2020-09-17T09:12:31.383851Z",
     "iopub.status.idle": "2020-09-17T09:12:58.762273Z",
     "shell.execute_reply": "2020-09-17T09:12:58.763277Z"
    },
    "papermill": {
     "duration": 27.461076,
     "end_time": "2020-09-17T09:12:58.763434",
     "exception": false,
     "start_time": "2020-09-17T09:12:31.302358",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "vec = TfidfVectorizer(stop_words=\"english\")\n",
    "vec.fit(df['clean_tweet'].values)\n",
    "features = vec.transform(df['clean_tweet'].values)\n",
    "\n",
    "#taking an arbitary value say 5, to get 5 cluster values\n",
    "kmeans = KMeans(n_clusters = 5,init ='k-means++', max_iter=300, random_state=0,verbose=1)\n",
    "y_kmeans =  kmeans.fit_predict(features)\n",
    "df['Cluster']  = y_kmeans\n",
    "#df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-09-17T09:12:58.957176Z",
     "iopub.status.busy": "2020-09-17T09:12:58.955996Z",
     "iopub.status.idle": "2020-09-17T09:12:58.958480Z",
     "shell.execute_reply": "2020-09-17T09:12:58.958976Z"
    },
    "papermill": {
     "duration": 0.087991,
     "end_time": "2020-09-17T09:12:58.959087",
     "exception": false,
     "start_time": "2020-09-17T09:12:58.871096",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#df[df['Cluster'] == 0].head(20)['text'].tolist()\n",
    "#df[df['Cluster'] == 1].head(20)['text'].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.080464,
     "end_time": "2020-09-17T09:12:59.126808",
     "exception": false,
     "start_time": "2020-09-17T09:12:59.046344",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "##### 7.1 Word-cloud of Cluster 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-09-17T09:12:59.295516Z",
     "iopub.status.busy": "2020-09-17T09:12:59.294007Z",
     "iopub.status.idle": "2020-09-17T09:12:59.705267Z",
     "shell.execute_reply": "2020-09-17T09:12:59.705708Z"
    },
    "papermill": {
     "duration": 0.498419,
     "end_time": "2020-09-17T09:12:59.705833",
     "exception": false,
     "start_time": "2020-09-17T09:12:59.207414",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "build_wordcloud(df[df['Cluster'] == 0]['text'], '1st word cluster using KMeans')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.090693,
     "end_time": "2020-09-17T09:12:59.884949",
     "exception": false,
     "start_time": "2020-09-17T09:12:59.794256",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "##### 7.2 Word-cloud of Cluster 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-09-17T09:13:00.064851Z",
     "iopub.status.busy": "2020-09-17T09:13:00.063987Z",
     "iopub.status.idle": "2020-09-17T09:13:00.283346Z",
     "shell.execute_reply": "2020-09-17T09:13:00.283789Z"
    },
    "papermill": {
     "duration": 0.31123,
     "end_time": "2020-09-17T09:13:00.283916",
     "exception": false,
     "start_time": "2020-09-17T09:12:59.972686",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "build_wordcloud(df[df['Cluster'] == 1]['text'], '2nd Word Cluster using KMeans')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.092717,
     "end_time": "2020-09-17T09:13:00.470671",
     "exception": false,
     "start_time": "2020-09-17T09:13:00.377954",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "##### 7.3 Word-cloud of Cluster 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-09-17T09:13:00.663757Z",
     "iopub.status.busy": "2020-09-17T09:13:00.662802Z",
     "iopub.status.idle": "2020-09-17T09:13:00.913500Z",
     "shell.execute_reply": "2020-09-17T09:13:00.913968Z"
    },
    "papermill": {
     "duration": 0.350526,
     "end_time": "2020-09-17T09:13:00.914093",
     "exception": false,
     "start_time": "2020-09-17T09:13:00.563567",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "build_wordcloud(df[df['Cluster'] == 2]['text'], '3rd word cluster using KMeans')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.099052,
     "end_time": "2020-09-17T09:13:01.113780",
     "exception": false,
     "start_time": "2020-09-17T09:13:01.014728",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "##### 7.4 Word-cloud of Cluster 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-09-17T09:13:01.319765Z",
     "iopub.status.busy": "2020-09-17T09:13:01.318869Z",
     "iopub.status.idle": "2020-09-17T09:13:01.549135Z",
     "shell.execute_reply": "2020-09-17T09:13:01.549571Z"
    },
    "papermill": {
     "duration": 0.336347,
     "end_time": "2020-09-17T09:13:01.549719",
     "exception": false,
     "start_time": "2020-09-17T09:13:01.213372",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "build_wordcloud(df[df['Cluster'] == 3]['text'], '4th word cluster using KMeans')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.105783,
     "end_time": "2020-09-17T09:13:01.763493",
     "exception": false,
     "start_time": "2020-09-17T09:13:01.657710",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "##### 7.5 Word-cloud of Cluster 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-09-17T09:13:01.984237Z",
     "iopub.status.busy": "2020-09-17T09:13:01.983316Z",
     "iopub.status.idle": "2020-09-17T09:13:02.217757Z",
     "shell.execute_reply": "2020-09-17T09:13:02.218177Z"
    },
    "papermill": {
     "duration": 0.347476,
     "end_time": "2020-09-17T09:13:02.218305",
     "exception": false,
     "start_time": "2020-09-17T09:13:01.870829",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "build_wordcloud(df[df['Cluster'] == 4]['text'], 'And finally the 5th word cluster using KMeans ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.112307,
     "end_time": "2020-09-17T09:13:02.463525",
     "exception": false,
     "start_time": "2020-09-17T09:13:02.351218",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### 8. Sentiment Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.113868,
     "end_time": "2020-09-17T09:13:02.688798",
     "exception": false,
     "start_time": "2020-09-17T09:13:02.574930",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "###### Here we shall use sentiments of  two types i.e. Postivie Sentiments and  Negative Sentiments and extracting sentiments from a pretrained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-09-17T09:13:02.923118Z",
     "iopub.status.busy": "2020-09-17T09:13:02.922410Z",
     "iopub.status.idle": "2020-09-17T09:13:39.812636Z",
     "shell.execute_reply": "2020-09-17T09:13:39.811538Z"
    },
    "papermill": {
     "duration": 37.011555,
     "end_time": "2020-09-17T09:13:39.812763",
     "exception": false,
     "start_time": "2020-09-17T09:13:02.801208",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "nltk.download('vader_lexicon')\n",
    "sid = SentimentIntensityAnalyzer()\n",
    "\n",
    "def get_score(text):\n",
    "    dict_res = sid.polarity_scores(text)\n",
    "    return dict_res[\"compound\"]\n",
    "\n",
    "df[\"Score\"] = df[\"clean_tweet\"].apply(lambda x: get_score(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-09-17T09:13:40.052613Z",
     "iopub.status.busy": "2020-09-17T09:13:40.048230Z",
     "iopub.status.idle": "2020-09-17T09:13:40.061032Z",
     "shell.execute_reply": "2020-09-17T09:13:40.060608Z"
    },
    "papermill": {
     "duration": 0.129515,
     "end_time": "2020-09-17T09:13:40.061127",
     "exception": false,
     "start_time": "2020-09-17T09:13:39.931612",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df[\"Score\"].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-09-17T09:13:40.293997Z",
     "iopub.status.busy": "2020-09-17T09:13:40.293168Z",
     "iopub.status.idle": "2020-09-17T09:13:40.317036Z",
     "shell.execute_reply": "2020-09-17T09:13:40.316571Z"
    },
    "papermill": {
     "duration": 0.14372,
     "end_time": "2020-09-17T09:13:40.317133",
     "exception": false,
     "start_time": "2020-09-17T09:13:40.173413",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "pred_df=pd.DataFrame({'text':df['clean_tweet'],'pred_sentiment':df['Score'],'country':df['country'],'text_length':df['text_length']})\n",
    "pred_df['pred_sentiment']=np.where(pred_df['pred_sentiment']>0.5,1,0)\n",
    "pred_df[['text','pred_sentiment']].head(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-09-17T09:13:40.553762Z",
     "iopub.status.busy": "2020-09-17T09:13:40.553192Z",
     "iopub.status.idle": "2020-09-17T09:13:56.363115Z",
     "shell.execute_reply": "2020-09-17T09:13:56.363544Z"
    },
    "papermill": {
     "duration": 15.931643,
     "end_time": "2020-09-17T09:13:56.363682",
     "exception": false,
     "start_time": "2020-09-17T09:13:40.432039",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "bird = np.array(Image.open('../input/covid-19-tweet-supporting-files/twitter_logo.jpg'))\n",
    "fig, (ax2, ax3) = plt.subplots(1, 2, figsize=[30, 15])\n",
    "wordcloud2 = WordCloud( background_color='white',mask= bird,colormap=\"Reds\",\n",
    "                        width=600,\n",
    "                        height=400).generate(\" \".join(pred_df[pred_df['pred_sentiment']==0]['text']))\n",
    "ax2.imshow(wordcloud2)\n",
    "ax2.axis('off')\n",
    "ax2.set_title('Negative Sentiment Bird',fontsize=35);\n",
    "\n",
    "wordcloud3 = WordCloud( background_color='white',mask= bird,colormap=\"Blues\",\n",
    "                        width=600,\n",
    "                        height=400).generate(\" \".join(pred_df[pred_df['pred_sentiment']==1]['text']))\n",
    "ax3.imshow(wordcloud3)\n",
    "ax3.axis('off')\n",
    "ax3.set_title('Positive Sentiment Bird',fontsize=35)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.127155,
     "end_time": "2020-09-17T09:13:56.619046",
     "exception": false,
     "start_time": "2020-09-17T09:13:56.491891",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### 9. Top 10 countries that had positive sentiment of tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-09-17T09:13:56.900064Z",
     "iopub.status.busy": "2020-09-17T09:13:56.894097Z",
     "iopub.status.idle": "2020-09-17T09:13:57.131163Z",
     "shell.execute_reply": "2020-09-17T09:13:57.131609Z"
    },
    "papermill": {
     "duration": 0.385447,
     "end_time": "2020-09-17T09:13:57.131736",
     "exception": false,
     "start_time": "2020-09-17T09:13:56.746289",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "sentiment_countries = pd.DataFrame()\n",
    "sentiment_countries[\"Score\"] = df[\"Score\"]\n",
    "sentiment_countries[\"country\"] = df[\"country\"]\n",
    "\n",
    "sentiment_countries = sentiment_countries.sort_values(by = \"Score\",ascending=False)\n",
    "sentiment_countries = sentiment_countries.groupby(\"country\").sum().sort_values(by = \"Score\",ascending=False)[:10]\n",
    "\n",
    "plt.figure(figsize=(9,10))\n",
    "sns.barplot(list(sentiment_countries.values.flatten()),sentiment_countries.index,)\n",
    "plt.title(\"Top 10 Location By Positive Score\")\n",
    "plt.xlabel(\"Sentiment Score\")\n",
    "plt.ylabel(\"Location\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.128803,
     "end_time": "2020-09-17T09:13:57.389937",
     "exception": false,
     "start_time": "2020-09-17T09:13:57.261134",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### 10. Data distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.129332,
     "end_time": "2020-09-17T09:13:57.647181",
     "exception": false,
     "start_time": "2020-09-17T09:13:57.517849",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "#### 10.1 Overview of User Sentiments among the top three countries that tweeted during 2019"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-09-17T09:13:57.913405Z",
     "iopub.status.busy": "2020-09-17T09:13:57.912480Z",
     "iopub.status.idle": "2020-09-17T09:13:57.947283Z",
     "shell.execute_reply": "2020-09-17T09:13:57.946786Z"
    },
    "papermill": {
     "duration": 0.171406,
     "end_time": "2020-09-17T09:13:57.947377",
     "exception": false,
     "start_time": "2020-09-17T09:13:57.775971",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_clean = df.drop(['user_description','user_location',\t'user_created','text','is_retweet','date', 'hashtags' ],axis=1)\n",
    "df_clean_19 = df_clean[(df_clean.created_year == 2019)]\n",
    "top_3 = df_clean_19[(df_clean_19.country == 'USA') | (df_clean_19.country == 'India') | (df_clean_19.country == 'UK')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-09-17T09:13:58.214675Z",
     "iopub.status.busy": "2020-09-17T09:13:58.213676Z",
     "iopub.status.idle": "2020-09-17T09:13:58.344976Z",
     "shell.execute_reply": "2020-09-17T09:13:58.345402Z"
    },
    "papermill": {
     "duration": 0.268055,
     "end_time": "2020-09-17T09:13:58.345535",
     "exception": false,
     "start_time": "2020-09-17T09:13:58.077480",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "fig = px.parallel_categories(top_3, dimensions=['country', 'user_verified','covid', 'created_month'],\n",
    "                color=\"Score\", color_continuous_scale=px.colors.sequential.Inferno,\n",
    "                labels={'sex':'Payer sex', 'smoker':'Smokers at the table', 'day':'Day of week'})\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.131816,
     "end_time": "2020-09-17T09:13:58.610823",
     "exception": false,
     "start_time": "2020-09-17T09:13:58.479007",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "##### 10.2 Overview of User Sentiments among the top three countries that tweeted during 2010"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-09-17T09:13:58.883979Z",
     "iopub.status.busy": "2020-09-17T09:13:58.882967Z",
     "iopub.status.idle": "2020-09-17T09:13:58.904401Z",
     "shell.execute_reply": "2020-09-17T09:13:58.903950Z"
    },
    "papermill": {
     "duration": 0.160787,
     "end_time": "2020-09-17T09:13:58.904508",
     "exception": false,
     "start_time": "2020-09-17T09:13:58.743721",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#df_clean = df.drop(['user_description','user_location',\t'user_created','text','is_retweet','date', 'hashtags' ],axis=1)\n",
    "df_clean_20 = df_clean[(df_clean.created_year == 2020)]\n",
    "top_3_20 = df_clean_20[(df_clean_20.country == 'USA') | (df_clean_20.country == 'India') | (df_clean_20.country == 'UK')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-09-17T09:13:59.192850Z",
     "iopub.status.busy": "2020-09-17T09:13:59.191971Z",
     "iopub.status.idle": "2020-09-17T09:13:59.310248Z",
     "shell.execute_reply": "2020-09-17T09:13:59.310710Z"
    },
    "papermill": {
     "duration": 0.2656,
     "end_time": "2020-09-17T09:13:59.310837",
     "exception": false,
     "start_time": "2020-09-17T09:13:59.045237",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "fig = px.parallel_categories(top_3_20, dimensions=['country', 'user_verified','covid', 'created_month'],\n",
    "                color=\"Score\", color_continuous_scale=px.colors.sequential.Inferno,\n",
    "                labels={'sex':'Payer sex', 'smoker':'Smokers at the table', 'day':'Day of week'})\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.135714,
     "end_time": "2020-09-17T09:13:59.583829",
     "exception": false,
     "start_time": "2020-09-17T09:13:59.448115",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "\n",
    "### <div align=\"center\">   With endless possibilites of representation and uncertainity, I hereby end this analysis with good hope!\n",
    "\n",
    "<div align=\"center\"> Thanks and Regards,\n",
    "<div align=\"center\"> Mohamed Riaz | mohamed.riaz1307@gmail.com \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": true,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "papermill": {
   "duration": 128.952446,
   "end_time": "2020-09-17T09:13:59.826523",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2020-09-17T09:11:50.874077",
   "version": "2.1.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
